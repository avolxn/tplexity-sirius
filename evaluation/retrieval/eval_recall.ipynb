{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ Retrieval: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "\n",
    "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞:\n",
    "- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∏–∑ `queries.json` (–ø–æ 5 —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç)\n",
    "- –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ç–æ–ø-10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
    "- –°—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è K = 1, 3, 5, 10\n",
    "\n",
    "## –ú–µ—Ç—Ä–∏–∫–∏\n",
    "1. **Hit Rate@K (Recall@K)** - –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ —è–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ–ø–∞–ª –≤ —Ç–æ–ø-K\n",
    "2. **MRR@K** (Mean Reciprocal Rank) - —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞ –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "3. **Pool-normalized Recall@K** - –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ top-K, –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ min(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ –ø—É–ª–µ, K)\n",
    "\n",
    "## –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è\n",
    "- **Ground truth**: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑–≤–µ—Å—Ç–µ–Ω —è–∫–æ—Ä–Ω—ã–π –ø–æ—Å—Ç (–∏–∑ `queries.json`)\n",
    "- –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞\n",
    "- –°—á–∏—Ç–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è K ‚àà {1, 3, 5, 10}\n",
    "\n",
    "## –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "- `report.txt` - –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç —Å–æ –≤—Å–µ–º–∏ –ª–æ–≥–∞–º–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "- `eval_results_multi_k.json` - –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tplexity.tg_parse.chunker import PostChunker\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å JSON\n",
    "with open(\"./eval_data/messages_diverse_1000posts_all_channels.json\", encoding=\"utf-8\") as f:\n",
    "    posts = json.load(f)  # –ú–æ–∂–µ—Ç –±—ã—Ç—å —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–µ—Ä\n",
    "chunker = PostChunker(\n",
    "    source_name=\"./eval_data/messages_diverse_1000posts_all_channels.json\", chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "# 3. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Å—Ç—ã –∏ —Å–æ–±—Ä–∞—Ç—å —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –ø–æ–ª–µ–π\n",
    "all_chunks = []\n",
    "for post in posts:\n",
    "    chunks = chunker.chunk_post(post)\n",
    "\n",
    "    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞\n",
    "    for chunk_data in chunks:\n",
    "        chunk_obj = {\n",
    "            # –í—Å–µ –ø–æ–ª—è –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞\n",
    "            \"id\": post.get(\"id\"),\n",
    "            \"channel_id\": post.get(\"channel_id\"),\n",
    "            \"link\": post.get(\"link\"),\n",
    "            \"date\": post.get(\"date\"),\n",
    "            \"views\": post.get(\"views\"),\n",
    "            \"forwards\": post.get(\"forwards\"),\n",
    "            \"edit_date\": post.get(\"edit_date\"),\n",
    "            \"has_media\": post.get(\"has_media\"),\n",
    "            \"media_type\": post.get(\"media_type\"),\n",
    "            \"chunk_index\": chunk_data[\"chunk_index\"],\n",
    "            \"text\": chunk_data[\"chunk_text\"],  # chunk_text -> text\n",
    "        }\n",
    "        all_chunks.append(chunk_obj)\n",
    "\n",
    "# 4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON\n",
    "with open(\"chunks_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ 1702 —á–∞–Ω–∫–æ–≤\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É\n",
    "project_root = Path(\"/srv/nlp1/eval_dir/T-bank_NLP_\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å\n",
    "report_path = project_root / \"src/eval/report.txt\"\n",
    "\n",
    "\n",
    "# –ö–ª–∞—Å—Å –¥–ª—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å\n",
    "class DualLogger:\n",
    "    def __init__(self, filepath):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filepath, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "\n",
    "# –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º stdout –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å\n",
    "sys.stdout = DualLogger(report_path)\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º logging –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ StreamHandler\n",
    "# FileHandler –ù–ï –Ω—É–∂–µ–Ω, —Ç–∞–∫ –∫–∞–∫ DualLogger —É–∂–µ –ø–∏—à–µ—Ç –≤ —Ñ–∞–π–ª\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # –ü–∏—à–µ—Ç –≤ sys.stdout -> DualLogger -> —Ñ–∞–π–ª + —Ç–µ—Ä–º–∏–Ω–∞–ª\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üöÄ –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê RETRIEVAL - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÅ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞: {project_root}\")\n",
    "print(f\"üìù –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from tplexity.retriever.retriever_service import RetrieverService\n",
    "\n",
    "print(\"‚úÖ –ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RetrieverService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RetrieverService\n",
    "print(\"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RetrieverService...\")\n",
    "retriever = RetrieverService()\n",
    "print(\"‚úÖ RetrieverService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ queries.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º queries\n",
    "queries_path = project_root / \"src/eval/eval_data/queries.json\"\n",
    "\n",
    "with open(queries_path, encoding=\"utf-8\") as f:\n",
    "    all_queries = json.load(f)\n",
    "\n",
    "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "\n",
    "# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ\n",
    "NUM_QUERIES = None  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–∏—Å–ª–æ –¥–ª—è —Ç–µ—Å—Ç–∞, –∏–ª–∏ None –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "queries_subset = all_queries[:NUM_QUERIES] if NUM_QUERIES else all_queries\n",
    "\n",
    "# –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω–¥–µ–∫—Å—ã 0,1,2,5,6,7,10,11,12,...\n",
    "# –ü–∞—Ç—Ç–µ—Ä–Ω: –±–µ—Ä–µ–º 3 —ç–ª–µ–º–µ–Ω—Ç–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º 2 (—É—Å–ª–æ–≤–∏–µ: –∏–Ω–¥–µ–∫—Å % 5 < 3)\n",
    "queries = [q for i, q in enumerate(queries_subset) if i % 5 < 1]\n",
    "\n",
    "print(f\"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–∏–Ω–¥–µ–∫—Å—ã 0,1,2,5,6,7,10,11,12...): {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "print(f\"üìà –≠—Ç–æ {len(queries) / len(queries_subset) * 100:.1f}% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞\")\n",
    "print(f\"\\n–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞: {queries[0]['query']}\")\n",
    "print(f\"Ground truth: channel={queries[0]['id_channel']}, message={queries[0]['id_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_query(\n",
    "    query_data: dict, retriever, top_k: int = 10, k_values: list = None, max_retries: int = 3\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º\n",
    "\n",
    "    Args:\n",
    "        query_data: –î–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ queries.json\n",
    "        retriever: RetrieverService\n",
    "        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å >= max(k_values))\n",
    "        k_values: –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π K –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫\n",
    "        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
    "\n",
    "    Returns:\n",
    "        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤—Å–µ—Ö K\n",
    "    \"\"\"\n",
    "    if k_values is None:\n",
    "        k_values = [1, 3, 5, 10]\n",
    "    query_text = query_data[\"query\"]\n",
    "    ground_truth_id = f\"{query_data['id_channel']}_{query_data['id_message']}\"\n",
    "\n",
    "    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Ç–æ–ø-K –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å retry\n",
    "    search_results = None\n",
    "    last_error = None\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            search_results = await retriever.search(query=query_text, top_k=top_k, top_n=top_k, use_rerank=False)\n",
    "            break  # –£—Å–ø–µ—à–Ω–æ - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ retry\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–æ–≤—Ç–æ—Ä—è—é...\")\n",
    "                await asyncio.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ retry\n",
    "            else:\n",
    "                logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}\")\n",
    "\n",
    "    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–µ—Ç\n",
    "    if search_results is None:\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        error_result = {\n",
    "            \"query\": query_text,\n",
    "            \"ground_truth_id\": ground_truth_id,\n",
    "            \"gt_position\": -1,\n",
    "            \"error\": str(last_error) if last_error else \"Unknown error\",\n",
    "            \"top_k_ids\": [],\n",
    "        }\n",
    "        for k in k_values:\n",
    "            error_result[f\"hit_rate@{k}\"] = 0.0\n",
    "            error_result[f\"mrr@{k}\"] = 0.0\n",
    "            error_result[f\"pool_recall@{k}\"] = 0.0\n",
    "        return error_result\n",
    "\n",
    "    if not search_results or len(search_results) == 0:\n",
    "        # –ü—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –Ω—É–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        empty_result = {\"query\": query_text, \"ground_truth_id\": ground_truth_id, \"gt_position\": -1, \"top_k_ids\": []}\n",
    "        for k in k_values:\n",
    "            empty_result[f\"hit_rate@{k}\"] = 0.0\n",
    "            empty_result[f\"mrr@{k}\"] = 0.0\n",
    "            empty_result[f\"pool_recall@{k}\"] = 0.0\n",
    "        return empty_result\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é ground truth –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö\n",
    "    gt_position = -1  # -1 –æ–∑–Ω–∞—á–∞–µ—Ç \"–Ω–µ –Ω–∞–π–¥–µ–Ω\"\n",
    "    for pos, (_doc_id, _score, _text, metadata) in enumerate(search_results, start=1):\n",
    "        doc_id_from_metadata = metadata.get(\"doc_id\", \"\") if metadata else \"\"\n",
    "        if doc_id_from_metadata == ground_truth_id:\n",
    "            gt_position = pos\n",
    "            break\n",
    "\n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    result = {\n",
    "        \"query\": query_text,\n",
    "        \"ground_truth_id\": ground_truth_id,\n",
    "        \"gt_position\": gt_position,\n",
    "        \"top_k_ids\": [metadata.get(\"doc_id\", \"\") if metadata else \"\" for _, _, _, metadata in search_results],\n",
    "    }\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ K\n",
    "    for k in k_values:\n",
    "        # 1. Hit Rate@K (Recall@K) - –Ω–∞—à–ª–∏ –ª–∏ —è–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ç–æ–ø-K\n",
    "        if gt_position > 0 and gt_position <= k:\n",
    "            hit_rate = 1.0\n",
    "        else:\n",
    "            hit_rate = 0.0\n",
    "        result[f\"hit_rate@{k}\"] = hit_rate\n",
    "\n",
    "        # 2. MRR@K - –æ–±—Ä–∞—Ç–Ω—ã–π —Ä–∞–Ω–≥ (–µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ç–æ–ø-K)\n",
    "        if gt_position > 0 and gt_position <= k:\n",
    "            mrr = 1.0 / gt_position\n",
    "        else:\n",
    "            mrr = 0.0\n",
    "        result[f\"mrr@{k}\"] = mrr\n",
    "\n",
    "        # 3. Pool-normalized Recall@K\n",
    "        # –£ –Ω–∞—Å –≤—Å–µ–≥–¥–∞ 1 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (—è–∫–æ—Ä–Ω—ã–π)\n",
    "        # Pool = —Ç–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞\n",
    "        # –§–æ—Ä–º—É–ª–∞: (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ —Ç–æ–ø-K) / min(–≤—Å–µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ –ø—É–ª–µ, K)\n",
    "\n",
    "        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ –ø—É–ª–µ (—Ç–æ–ø-10): 1 –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω, 0 –µ—Å–ª–∏ –Ω–µ—Ç\n",
    "        relevant_in_pool = 1 if gt_position > 0 else 0\n",
    "\n",
    "        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ —Ç–æ–ø-K: 1 –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ç–æ–ø-K, 0 –∏–Ω–∞—á–µ\n",
    "        relevant_in_topk = 1 if (gt_position > 0 and gt_position <= k) else 0\n",
    "\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        if relevant_in_pool > 0:\n",
    "            # min(1, K) = 1 –¥–ª—è –ª—é–±–æ–≥–æ K >= 1\n",
    "            pool_recall = relevant_in_topk / min(relevant_in_pool, k)\n",
    "        else:\n",
    "            pool_recall = 0.0\n",
    "\n",
    "        result[f\"pool_recall@{k}\"] = pool_recall\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è evaluate_query –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û—Ü–µ–Ω–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [02:40<00:00,  6.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "TOP_K = 10  # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "K_VALUES = [1, 3, 5, 10]  # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —ç—Ç–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π K\n",
    "\n",
    "print(f\"üöÄ –ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "print(f\"üìä –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-{TOP_K} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è K = {K_VALUES}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–∞–∂–¥–æ–º—É K\n",
    "all_results = []\n",
    "metrics_by_k = {k: {\"hit_rates\": [], \"mrrs\": [], \"pool_recalls\": []} for k in K_VALUES}\n",
    "\n",
    "# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å\n",
    "for i, query_data in enumerate(tqdm(queries, desc=\"–û—Ü–µ–Ω–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤\")):\n",
    "    result = await evaluate_query(query_data, retriever, top_k=TOP_K, k_values=K_VALUES)\n",
    "\n",
    "    all_results.append(result)\n",
    "\n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ K\n",
    "    for k in K_VALUES:\n",
    "        metrics_by_k[k][\"hit_rates\"].append(result[f\"hit_rate@{k}\"])\n",
    "        metrics_by_k[k][\"mrrs\"].append(result[f\"mrr@{k}\"])\n",
    "        metrics_by_k[k][\"pool_recalls\"].append(result[f\"pool_recall@{k}\"])\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 100-–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"\\n[{i + 1}/{len(queries)}] –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "        for k in K_VALUES:\n",
    "            current_hit_rate = sum(metrics_by_k[k][\"hit_rates\"]) / len(metrics_by_k[k][\"hit_rates\"])\n",
    "            current_mrr = sum(metrics_by_k[k][\"mrrs\"]) / len(metrics_by_k[k][\"mrrs\"])\n",
    "            print(f\"  K={k}: Hit Rate={current_hit_rate:.4f} ({current_hit_rate * 100:.2f}%), MRR={current_mrr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ K\n",
    "final_metrics = {}\n",
    "for k in K_VALUES:\n",
    "    hit_rate = sum(metrics_by_k[k][\"hit_rates\"]) / len(metrics_by_k[k][\"hit_rates\"])\n",
    "    mrr = sum(metrics_by_k[k][\"mrrs\"]) / len(metrics_by_k[k][\"mrrs\"])\n",
    "    pool_recall = sum(metrics_by_k[k][\"pool_recalls\"]) / len(metrics_by_k[k][\"pool_recalls\"])\n",
    "\n",
    "    final_metrics[k] = {\n",
    "        \"hit_rate\": hit_rate,\n",
    "        \"mrr\": mrr,\n",
    "        \"pool_recall\": pool_recall,\n",
    "        \"num_hits\": sum(metrics_by_k[k][\"hit_rates\"]),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω\n",
    "    }\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}\")\n",
    "print(f\"–ü–æ–ª—É—á–µ–Ω–æ —Ç–æ–ø-{TOP_K} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\")\n",
    "print(f\"–ó–Ω–∞—á–µ–Ω–∏—è K –¥–ª—è –æ—Ü–µ–Ω–∫–∏: {K_VALUES}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫\n",
    "print(\"\\nüéØ –ú–ï–¢–†–ò–ö–ò –ü–û –ö–ê–ñ–î–û–ú–£ K:\\n\")\n",
    "print(f\"{'K':>4} | {'Hit Rate@K':>12} | {'MRR@K':>8} | {'Pool Recall@K':>15} | {'–ù–∞–π–¥–µ–Ω–æ':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for k in K_VALUES:\n",
    "    m = final_metrics[k]\n",
    "    print(\n",
    "        f\"{k:>4} | {m['hit_rate']:>11.4f} | {m['mrr']:>8.4f} | {m['pool_recall']:>15.4f} | {int(m['num_hits']):>4}/{len(queries)}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìù –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫:\")\n",
    "print(\"  ‚Ä¢ Hit Rate@K (Recall@K): –¥–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ —è–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ–ø–∞–ª –≤ —Ç–æ–ø-K\")\n",
    "print(\"  ‚Ä¢ MRR@K: —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 1/–ø–æ–∑–∏—Ü–∏—è –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–æ–ø-K (–≤—ã—à–µ = –ª—É—á—à–µ)\")\n",
    "print(\"  ‚Ä¢ Pool Recall@K: –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ç–æ–ø-K (–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "from collections import Counter\n",
    "\n",
    "# –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–ø-10\n",
    "found_in_top10 = sum(1 for r in all_results if r.get(\"gt_position\", -1) > 0)\n",
    "not_found = len(queries) - found_in_top10\n",
    "\n",
    "print(f\"\\nüìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—Ç–æ–ø-{TOP_K}):\")\n",
    "print(\n",
    "    f\"  –Ø–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ —Ç–æ–ø-{TOP_K}: {found_in_top10}/{len(queries)} ({found_in_top10 / len(queries) * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"  –Ø–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ù–ï –Ω–∞–π–¥–µ–Ω: {not_found}/{len(queries)} ({not_found / len(queries) * 100:.1f}%)\")\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π GT –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "positions = [r[\"gt_position\"] for r in all_results if r[\"gt_position\"] > 0]\n",
    "\n",
    "if positions:\n",
    "    distribution = Counter(positions)\n",
    "    print(\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —è–∫–æ—Ä–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∫–æ–≥–¥–∞ –Ω–∞–π–¥–µ–Ω):\")\n",
    "    for pos in sorted(distribution.keys()):\n",
    "        freq = distribution[pos]\n",
    "        print(f\"  –ü–æ–∑–∏—Ü–∏—è {pos}: {freq} –∑–∞–ø—Ä–æ—Å–æ–≤ ({freq / found_in_top10 * 100:.1f}% –æ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö)\")\n",
    "\n",
    "    avg_position = sum(positions) / len(positions)\n",
    "    median_position = sorted(positions)[len(positions) // 2]\n",
    "    print(f\"\\n  üìç –°—Ä–µ–¥–Ω—è—è –ø–æ–∑–∏—Ü–∏—è: {avg_position:.2f}\")\n",
    "    print(f\"  üìç –ú–µ–¥–∏–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è: {median_position}\")\n",
    "\n",
    "# –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É K\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û K:\")\n",
    "print(\"=\" * 80)\n",
    "for k in K_VALUES:\n",
    "    found_in_k = int(final_metrics[k][\"num_hits\"])\n",
    "    not_in_k = len(queries) - found_in_k\n",
    "    print(f\"\\nüîπ K = {k}:\")\n",
    "    print(f\"   –ù–∞–π–¥–µ–Ω–æ: {found_in_k} ({found_in_k / len(queries) * 100:.1f}%)\")\n",
    "    print(f\"   –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {not_in_k} ({not_in_k / len(queries) * 100:.1f}%)\")\n",
    "    print(f\"   Hit Rate@{k}: {final_metrics[k]['hit_rate']:.4f}\")\n",
    "    print(f\"   MRR@{k}: {final_metrics[k]['mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã –ü–†–ò–ú–ï–†–´ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –£—Å–ø–µ—à–Ω—ã–µ –∏ –Ω–µ—É–¥–∞—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "successful = [r for r in all_results if r[\"gt_position\"] > 0]\n",
    "failed = [r for r in all_results if r[\"gt_position\"] == -1]\n",
    "\n",
    "print(f\"\\n‚úÖ –ü—Ä–∏–º–µ—Ä—ã –£–°–ü–ï–®–ù–´–• –∑–∞–ø—Ä–æ—Å–æ–≤ (—è–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤ —Ç–æ–ø-{TOP_K}):\")\n",
    "print(\"-\" * 80)\n",
    "for i, result in enumerate(successful[:3], 1):\n",
    "    print(f\"\\n[–ü—Ä–∏–º–µ—Ä {i}]\")\n",
    "    print(f\"Query: {result['query'][:100]}...\")\n",
    "    print(f\"–Ø–∫–æ—Ä–Ω—ã–π doc_id: {result['ground_truth_id']}\")\n",
    "    print(f\"–ü–æ–∑–∏—Ü–∏—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö: {result['gt_position']}\")\n",
    "    print(\n",
    "        f\"–ú–µ—Ç—Ä–∏–∫–∏: Hit@1={result['hit_rate@1']:.0f}, Hit@3={result['hit_rate@3']:.0f}, Hit@5={result['hit_rate@5']:.0f}, Hit@10={result['hit_rate@10']:.0f}\"\n",
    "    )\n",
    "    print(f\"MRR@10: {result['mrr@10']:.4f}\")\n",
    "    print(f\"–¢–æ–ø-5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö doc_ids: {result['top_k_ids'][:5]}\")\n",
    "\n",
    "print(f\"\\n\\n‚ùå –ü—Ä–∏–º–µ—Ä—ã –ù–ï–£–î–ê–ß–ù–´–• –∑–∞–ø—Ä–æ—Å–æ–≤ (—è–∫–æ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ù–ï –Ω–∞–π–¥–µ–Ω –≤ —Ç–æ–ø-{TOP_K}):\")\n",
    "print(\"-\" * 80)\n",
    "for i, result in enumerate(failed[:3], 1):\n",
    "    print(f\"\\n[–ü—Ä–∏–º–µ—Ä {i}]\")\n",
    "    print(f\"Query: {result['query'][:100]}...\")\n",
    "    print(f\"–Ø–∫–æ—Ä–Ω—ã–π doc_id: {result['ground_truth_id']}\")\n",
    "    print(f\"–¢–æ–ø-5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö doc_ids: {result['top_k_ids'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/nlp1/eval_dir/T-bank_NLP_/.venv/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py\", line 565, in _log_error\n",
      "    f.result()\n",
      "  File \"/srv/nlp1/eval_dir/T-bank_NLP_/.venv/lib/python3.12/site-packages/ipykernel/utils.py\", line 60, in run_in_context\n",
      "    return await asyncio.create_task(coro, context=context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/nlp1/eval_dir/T-bank_NLP_/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/srv/nlp1/eval_dir/T-bank_NLP_/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 484, in dispatch_shell\n",
      "    sys.stdout.flush()\n",
      "  File \"/tmp/ipykernel_3871708/3091458877.py\", line 29, in flush\n",
      "    self.log.flush()\n",
      "ValueError: I/O operation on closed file.\n"
     ]
    }
   ],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON\n",
    "# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç—Ä–∏–∫\n",
    "metrics_dict = {}\n",
    "for k in K_VALUES:\n",
    "    metrics_dict[f\"hit_rate@{k}\"] = final_metrics[k][\"hit_rate\"]\n",
    "    metrics_dict[f\"mrr@{k}\"] = final_metrics[k][\"mrr\"]\n",
    "    metrics_dict[f\"pool_recall@{k}\"] = final_metrics[k][\"pool_recall\"]\n",
    "    metrics_dict[f\"num_hits@{k}\"] = int(final_metrics[k][\"num_hits\"])\n",
    "\n",
    "output_data = {\n",
    "    \"config\": {\n",
    "        \"num_queries\": len(queries),\n",
    "        \"top_k\": TOP_K,\n",
    "        \"k_values\": K_VALUES,\n",
    "        \"use_rerank\": False,\n",
    "        \"filter_pattern\": \"indices 0,1,2,5,6,7,... (i % 5 < 3)\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    },\n",
    "    \"metrics\": metrics_dict,\n",
    "    \"summary\": {\n",
    "        \"total_queries\": len(queries),\n",
    "        \"found_in_top10\": found_in_top10,\n",
    "        \"not_found\": not_found,\n",
    "        \"avg_position\": avg_position if positions else None,\n",
    "        \"median_position\": median_position if positions else None,\n",
    "    },\n",
    "    \"detailed_results\": all_results,\n",
    "}\n",
    "\n",
    "output_path = project_root / \"src/eval/eval_results_multi_k.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}\")\n",
    "print(\"\\nüìä –°–≤–æ–¥–∫–∞ –∏—Ç–æ–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫:\")\n",
    "for k in K_VALUES:\n",
    "    print(\n",
    "        f\"  K={k}: Hit Rate={final_metrics[k]['hit_rate']:.4f}, MRR={final_metrics[k]['mrr']:.4f}, Pool Recall={final_metrics[k]['pool_recall']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìù –ü–û–õ–ù–´–ô –û–¢–ß–Å–¢ –°–û–•–†–ê–ù–Å–ù –í: {report_path}\")\n",
    "print(f\"üìã JSON –†–ï–ó–£–õ–¨–¢–ê–¢–´: {output_path}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ –û—Ç—á—ë—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω!\")\n",
    "\n",
    "# –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "if hasattr(sys.stdout, \"log\"):\n",
    "    sys.stdout.log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
