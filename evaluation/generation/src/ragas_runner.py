"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–∑ Ragas –∏ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.
"""

import asyncio
import logging
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ tplexity
project_root = Path(__file__).parent.parent.parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

try:
    from .custom_metrics import (
        JudgeClient,
        score_relevance,
        score_faithfulness,
        score_completeness,
        score_all_metrics,
        score_all_metrics_async
    )
except ImportError:
    from custom_metrics import (
        JudgeClient,
        score_relevance,
        score_faithfulness,
        score_completeness,
        score_all_metrics,
        score_all_metrics_async
    )

logger = logging.getLogger(__name__)


def _print_intermediate_metrics(results: List[Dict[str, Any]], current_idx: int, total: int):
    """
    –í—ã–≤–æ–¥–∏—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
        current_idx: –¢–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å (0-based)
        total: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
    """
    if not results:
        return
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ —Ç–µ–∫—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df = pd.DataFrame(results)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞
    metrics = ["relevance", "faithfulness", "completeness", "off_topic_rate", "latency_ms"]
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    metric_values = {}
    for metric in metrics:
        if metric in df.columns:
            values = df[metric].dropna()
            if len(values) > 0:
                metric_values[metric] = {
                    "mean": float(values.mean()),
                    "std": float(values.std())
                }
    
    # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    logger.info("=" * 80)
    logger.info(f"üìä –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {current_idx + 1}/{total}):")
    logger.info("-" * 80)
    for metric in metrics:
        if metric in metric_values:
            mean = metric_values[metric]["mean"]
            std = metric_values[metric]["std"]
            logger.info(f"  {metric:20s}: {mean:.4f} ¬± {std:.4f}")
    logger.info("=" * 80)


# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Ragas
try:
    from ragas import evaluate
    from ragas.metrics import (
        answer_relevancy,
        faithfulness,
        context_precision,
        context_recall
    )
    RAGAS_AVAILABLE = True
    logger.info("Ragas –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    RAGAS_AVAILABLE = False
    logger.warning("Ragas –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å LangChain –¥–ª—è –æ–±–µ—Ä—Ç–∫–∏ LLM
try:
    from langchain_openai import ChatOpenAI
    from ragas.llms import LangchainLLMWrapper
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    logger.debug("LangChain –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–±–µ—Ä—Ç–∫–∏ LLM, Ragas –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")


def _create_ragas_llm(judge_config: Dict[str, Any]):
    """
    –°–æ–∑–¥–∞–µ—Ç LLM –¥–ª—è Ragas –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ judge.
    
    Args:
        judge_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è judge –º–æ–¥–µ–ª–∏
        
    Returns:
        LLM –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è Ragas –∏–ª–∏ None (–µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å)
    """
    provider = judge_config.get("provider", "qwen").lower()
    
    if provider == "qwen":
        try:
            from tplexity.llm_client.config import settings as llm_settings
            
            # –°–æ–∑–¥–∞–µ–º LangChain LLM –¥–ª—è Qwen —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
            if LANGCHAIN_AVAILABLE:
                llm = ChatOpenAI(
                    model=llm_settings.qwen_model,
                    api_key=llm_settings.qwen_api_key,
                    base_url=llm_settings.qwen_base_url,
                    temperature=judge_config.get("temperature", 0.0),
                    timeout=judge_config.get("timeout", 30)
                )
                ragas_llm = LangchainLLMWrapper(llm)
                logger.info(f"–°–æ–∑–¥–∞–Ω Ragas LLM –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è Qwen: {llm_settings.qwen_model}")
                return ragas_llm
            else:
                logger.warning("LangChain –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, Ragas –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                return None
                
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Ragas LLM –¥–ª—è Qwen: {e}")
            return None
    
    elif provider == "yandexgpt":
        try:
            from tplexity.llm_client.config import settings as llm_settings
            
            # –°–æ–∑–¥–∞–µ–º LangChain LLM –¥–ª—è YandexGPT —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API
            if LANGCHAIN_AVAILABLE:
                # YandexGPT —Ç—Ä–µ–±—É–µ—Ç folder_id –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
                model_name = f"gpt://{llm_settings.yandexgpt_folder_id}/{llm_settings.yandexgpt_model}"
                llm = ChatOpenAI(
                    model=model_name,
                    api_key=llm_settings.yandexgpt_api_key,
                    base_url=llm_settings.yandexgpt_base_url,
                    temperature=judge_config.get("temperature", 0.0),
                    timeout=judge_config.get("timeout", 30),
                    default_headers={"x-folder-id": llm_settings.yandexgpt_folder_id}
                )
                ragas_llm = LangchainLLMWrapper(llm)
                logger.info(f"–°–æ–∑–¥–∞–Ω Ragas LLM –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è YandexGPT: {llm_settings.yandexgpt_model}")
                return ragas_llm
            else:
                logger.warning("LangChain –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, Ragas –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                return None
                
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Ragas LLM –¥–ª—è YandexGPT: {e}")
            return None
    
    elif provider == "openai":
        try:
            api_key = judge_config.get("api_key") or os.getenv("OPENAI_API_KEY")
            model = judge_config.get("model", "gpt-4o-mini")
            
            if LANGCHAIN_AVAILABLE and api_key:
                llm = ChatOpenAI(
                    model=model,
                    api_key=api_key,
                    temperature=judge_config.get("temperature", 0.0),
                    timeout=judge_config.get("timeout", 30)
                )
                ragas_llm = LangchainLLMWrapper(llm)
                logger.info(f"–°–æ–∑–¥–∞–Ω Ragas LLM –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è OpenAI: {model}")
                return ragas_llm
            else:
                logger.warning("OpenAI API key –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ LangChain –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return None
                
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Ragas LLM –¥–ª—è OpenAI: {e}")
            return None
    
    else:
        logger.warning(f"–ü—Ä–æ–≤–∞–π–¥–µ—Ä {provider} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è Ragas LLM")
        return None


async def run_evaluation_async(
    examples: List[Dict[str, Any]],
    judge_config: Dict[str, Any],
    use_ragas: bool = True,
    batch_size: int = 10
) -> pd.DataFrame:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è run_evaluation —Å –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    
    Args:
        examples: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ–ª—è–º–∏ question, contexts, answer, cited_sources, latency_ms
        judge_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è judge –º–æ–¥–µ–ª–∏
        use_ragas: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Ragas (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
    Returns:
        DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    """
    if use_ragas and RAGAS_AVAILABLE:
        return await _run_ragas_evaluation_async(examples, judge_config, batch_size)
    else:
        return await _run_custom_evaluation_async(examples, judge_config, batch_size)


async def _run_ragas_evaluation_async(
    examples: List[Dict[str, Any]],
    judge_config: Dict[str, Any],
    batch_size: int
) -> pd.DataFrame:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è _run_ragas_evaluation —Å –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    """
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–∑ Ragas –¥–ª—è {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ (batch_size={batch_size})")
    
    # –°–æ–∑–¥–∞–µ–º LLM –¥–ª—è Ragas (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —á–∞—Å—Ç—å)
    ragas_llm = _create_ragas_llm(judge_config)
    if ragas_llm:
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞—Å—Ç–æ–º–Ω–∞—è LLM –¥–ª—è Ragas")
    else:
        logger.info("Ragas –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—É—é LLM (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è)")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Ragas
    ragas_data = []
    for ex in examples:
        ragas_data.append({
            "question": ex["question"],
            "contexts": ex["contexts"],
            "answer": ex["answer"],
            "ground_truth": ""
        })
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º Ragas –º–µ—Ç—Ä–∏–∫–∏ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —á–∞—Å—Ç—å, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
    try:
        ragas_df = pd.DataFrame(ragas_data)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —á–∞—Å—Ç—å)
        if ragas_llm:
            metrics = []
            metric_classes = [
                (answer_relevancy, "answer_relevancy"),
                (faithfulness, "faithfulness"),
                (context_precision, "context_precision"),
                (context_recall, "context_recall")
            ]
            
            for metric_class, metric_name in metric_classes:
                try:
                    if callable(metric_class) and not isinstance(metric_class, type):
                        metric = metric_class(llm=ragas_llm)
                    elif isinstance(metric_class, type):
                        metric = metric_class(llm=ragas_llm)
                    else:
                        metric = metric_class
                        if hasattr(metric, 'llm'):
                            metric.llm = ragas_llm
                        elif hasattr(metric, '_llm'):
                            metric._llm = ragas_llm
                    metrics.append(metric)
                except (TypeError, AttributeError) as e:
                    logger.debug(f"–ú–µ—Ç—Ä–∏–∫–∞ {metric_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç llm –ø–∞—Ä–∞–º–µ—Ç—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è: {e}")
                    metrics.append(metric_class)
            
            if any(hasattr(m, 'llm') or hasattr(m, '_llm') for m in metrics if hasattr(m, '__dict__')):
                logger.info("–ú–µ—Ç—Ä–∏–∫–∏ Ragas –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π LLM")
        else:
            metrics = [
                answer_relevancy,
                faithfulness,
                context_precision,
                context_recall
            ]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º Ragas –æ—Ü–µ–Ω–∫—É (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —á–∞—Å—Ç—å)
        evaluate_kwargs = {
            "dataset": ragas_df,
            "metrics": metrics
        }
        
        if ragas_llm:
            import inspect
            try:
                sig = inspect.signature(evaluate)
                if 'llm' in sig.parameters:
                    evaluate_kwargs['llm'] = ragas_llm
                    logger.info("LLM –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ evaluate()")
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–¥–∞—Ç—å LLM –≤ evaluate: {e}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º Ragas –æ—Ü–µ–Ω–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        ragas_results = await asyncio.to_thread(evaluate, **evaluate_kwargs)
        ragas_metrics_df = ragas_results.to_pandas()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Ragas: {e}")
        logger.warning("–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
        return await _run_custom_evaluation_async(examples, judge_config, batch_size)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –±–∞—Ç—á–∞–º–∏
    judge_client = JudgeClient(
        provider=judge_config.get("provider", "qwen"),
        model=judge_config.get("model", ""),
        api_key=judge_config.get("api_key"),
        temperature=judge_config.get("temperature", 0.0),
        max_retries=judge_config.get("max_retries", 2),
        timeout=judge_config.get("timeout", 30)
    )
    
    async def process_single_example(idx: int, ex: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
        result_row = {
            "query_id": idx,
            "question": ex["question"],
            "n_contexts": len(ex["contexts"]),
            "latency_ms": ex.get("latency_ms", 0.0)
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ Ragas
        if idx < len(ragas_metrics_df):
            result_row["relevance"] = ragas_metrics_df.iloc[idx].get("answer_relevancy", 0.0)
            result_row["faithfulness"] = ragas_metrics_df.iloc[idx].get("faithfulness", 0.0)
            result_row["context_precision"] = ragas_metrics_df.iloc[idx].get("context_precision", 0.0)
            result_row["context_recall"] = ragas_metrics_df.iloc[idx].get("context_recall", 0.0)
        else:
            result_row["relevance"] = 0.0
            result_row["faithfulness"] = 0.0
            result_row["context_precision"] = 0.0
            result_row["context_recall"] = 0.0
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        cited_sources = ex.get("cited_sources", [])
        
        if result_row.get("faithfulness", 0.0) > 0.0:
            _, _, completeness, off_topic_rate, has_error = await score_all_metrics_async(
                judge_client, ex["question"], ex["answer"], ex["contexts"], cited_sources
            )
        else:
            relevance_custom, faithfulness_custom, completeness, off_topic_rate, has_error = await score_all_metrics_async(
                judge_client, ex["question"], ex["answer"], ex["contexts"], cited_sources
            )
            result_row["relevance"] = relevance_custom
            result_row["faithfulness"] = faithfulness_custom
        
        result_row["completeness"] = completeness
        result_row["off_topic_rate"] = off_topic_rate
        result_row["judge_errors"] = has_error
        
        return result_row
    
    async def process_batch(batch_examples: List[tuple[int, Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
        tasks = [process_single_example(idx, ex) for idx, ex in batch_examples]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        batch_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞: {result}")
            else:
                batch_results.append(result)
        
        return batch_results
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –±–∞—Ç—á–∞–º–∏
    results = []
    from tqdm import tqdm
    
    with tqdm(total=len(examples), desc="–û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫") as pbar:
        for i in range(0, len(examples), batch_size):
            batch = [(idx, ex) for idx, ex in enumerate(examples[i:i + batch_size], start=i)]
            batch_results = await process_batch(batch)
            results.extend(batch_results)
            pbar.update(len(batch))
    
    return pd.DataFrame(results)


async def _run_custom_evaluation_async(
    examples: List[Dict[str, Any]],
    judge_config: Dict[str, Any],
    batch_size: int
) -> pd.DataFrame:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è _run_custom_evaluation —Å –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–æ–π.
    """
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ (batch_size={batch_size})")
    
    judge_client = JudgeClient(
        provider=judge_config.get("provider", "qwen"),
        model=judge_config.get("model", ""),
        api_key=judge_config.get("api_key"),
        temperature=judge_config.get("temperature", 0.0),
        max_retries=judge_config.get("max_retries", 2),
        timeout=judge_config.get("timeout", 30)
    )
    
    async def process_single_example(idx: int, ex: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
        result_row = {
            "query_id": idx,
            "question": ex["question"],
            "n_contexts": len(ex["contexts"]),
            "latency_ms": ex.get("latency_ms", 0.0)
        }
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        cited_sources = ex.get("cited_sources", [])
        relevance_score, faithfulness_score, completeness_score, off_topic_rate, has_error = await score_all_metrics_async(
            judge_client, ex["question"], ex["answer"], ex["contexts"], cited_sources
        )
        
        result_row["relevance"] = relevance_score
        result_row["faithfulness"] = faithfulness_score
        result_row["completeness"] = completeness_score
        result_row["off_topic_rate"] = off_topic_rate
        result_row["context_precision"] = faithfulness_score  # –ò—Å–ø–æ–ª—å–∑—É–µ–º faithfulness –∫–∞–∫ proxy
        result_row["context_recall"] = min(1.0, len(ex["contexts"]) / 5.0)  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
        result_row["judge_errors"] = has_error
        
        return result_row
    
    async def process_batch(batch_examples: List[tuple[int, Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
        tasks = [process_single_example(idx, ex) for idx, ex in batch_examples]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        batch_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞: {result}")
            else:
                batch_results.append(result)
        
        return batch_results
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –±–∞—Ç—á–∞–º–∏
    results = []
    from tqdm import tqdm
    
    with tqdm(total=len(examples), desc="–û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫") as pbar:
        for i in range(0, len(examples), batch_size):
            batch = [(idx, ex) for idx, ex in enumerate(examples[i:i + batch_size], start=i)]
            batch_results = await process_batch(batch)
            results.extend(batch_results)
            pbar.update(len(batch))
    
    return pd.DataFrame(results)


def run_evaluation(
    examples: List[Dict[str, Any]],
    judge_config: Dict[str, Any],
    use_ragas: bool = True
) -> pd.DataFrame:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö.
    
    Args:
        examples: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ–ª—è–º–∏ question, contexts, answer, cited_sources, latency_ms
        judge_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è judge –º–æ–¥–µ–ª–∏
        use_ragas: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Ragas (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        
    Returns:
        DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    """
    if use_ragas and RAGAS_AVAILABLE:
        return _run_ragas_evaluation(examples, judge_config)
    else:
        return _run_custom_evaluation(examples, judge_config)


def _run_ragas_evaluation(
    examples: List[Dict[str, Any]],
    judge_config: Dict[str, Any]
) -> pd.DataFrame:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑ Ragas —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫.
    """
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–∑ Ragas –¥–ª—è {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º LLM –¥–ª—è Ragas
    ragas_llm = _create_ragas_llm(judge_config)
    if ragas_llm:
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞—Å—Ç–æ–º–Ω–∞—è LLM –¥–ª—è Ragas")
    else:
        logger.info("Ragas –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—É—é LLM (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è)")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Ragas
    ragas_data = []
    for ex in examples:
        ragas_data.append({
            "question": ex["question"],
            "contexts": ex["contexts"],
            "answer": ex["answer"],
            "ground_truth": ""  # Ragas —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ –ø–æ–ª–µ, –Ω–æ –º—ã –µ–≥–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º
        })
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º Ragas –º–µ—Ç—Ä–∏–∫–∏
    try:
        ragas_df = pd.DataFrame(ragas_data)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å LLM, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        # –í Ragas –º–µ—Ç—Ä–∏–∫–∏ –æ–±—ã—á–Ω–æ –º–æ–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º llm
        if ragas_llm:
            # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π LLM
            # –í Ragas –º–µ—Ç—Ä–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–ª–∞—Å—Å–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç llm –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            metrics = []
            metric_classes = [
                (answer_relevancy, "answer_relevancy"),
                (faithfulness, "faithfulness"),
                (context_precision, "context_precision"),
                (context_recall, "context_recall")
            ]
            
            for metric_class, metric_name in metric_classes:
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É —Å LLM
                    if callable(metric_class) and not isinstance(metric_class, type):
                        # –ï—Å–ª–∏ —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è/—Ñ–∞–±—Ä–∏–∫–∞
                        metric = metric_class(llm=ragas_llm)
                    elif isinstance(metric_class, type):
                        # –ï—Å–ª–∏ —ç—Ç–æ –∫–ª–∞—Å—Å
                        metric = metric_class(llm=ragas_llm)
                    else:
                        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ —ç–∫–∑–µ–º–ø–ª—è—Ä, –ø—ã—Ç–∞–µ–º—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å LLM
                        metric = metric_class
                        if hasattr(metric, 'llm'):
                            metric.llm = ragas_llm
                        elif hasattr(metric, '_llm'):
                            metric._llm = ragas_llm
                    metrics.append(metric)
                except (TypeError, AttributeError) as e:
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å LLM, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–µ—Ç—Ä–∏–∫—É
                    logger.debug(f"–ú–µ—Ç—Ä–∏–∫–∞ {metric_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç llm –ø–∞—Ä–∞–º–µ—Ç—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è: {e}")
                    metrics.append(metric_class)
            
            if any(hasattr(m, 'llm') or hasattr(m, '_llm') for m in metrics if hasattr(m, '__dict__')):
                logger.info("–ú–µ—Ç—Ä–∏–∫–∏ Ragas –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π LLM")
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –±–µ–∑ –∫–∞—Å—Ç–æ–º–Ω–æ–π LLM
            metrics = [
                answer_relevancy,
                faithfulness,
                context_precision,
                context_recall
            ]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
        evaluate_kwargs = {
            "dataset": ragas_df,
            "metrics": metrics
        }
        
        # –ï—Å–ª–∏ Ragas –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É LLM –Ω–∞–ø—Ä—è–º—É—é –≤ evaluate
        if ragas_llm:
            import inspect
            try:
                sig = inspect.signature(evaluate)
                if 'llm' in sig.parameters:
                    evaluate_kwargs['llm'] = ragas_llm
                    logger.info("LLM –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ evaluate()")
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–¥–∞—Ç—å LLM –≤ evaluate: {e}")
        
        ragas_results = evaluate(**evaluate_kwargs)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Ragas –≤ DataFrame
        ragas_metrics_df = ragas_results.to_pandas()
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Ragas: {e}")
        logger.warning("–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
        return _run_custom_evaluation(examples, judge_config)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ judge
    judge_client = JudgeClient(
        provider=judge_config.get("provider", "qwen"),
        model=judge_config.get("model", ""),
        api_key=judge_config.get("api_key"),
        temperature=judge_config.get("temperature", 0.0),
        max_retries=judge_config.get("max_retries", 2),
        timeout=judge_config.get("timeout", 30)
    )
    
    results = []
    for idx, ex in enumerate(examples):
        result_row = {
            "query_id": idx,
            "question": ex["question"],
            "n_contexts": len(ex["contexts"]),
            "latency_ms": ex.get("latency_ms", 0.0)
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ Ragas
        if idx < len(ragas_metrics_df):
            result_row["relevance"] = ragas_metrics_df.iloc[idx].get("answer_relevancy", 0.0)
            result_row["faithfulness"] = ragas_metrics_df.iloc[idx].get("faithfulness", 0.0)
            result_row["context_precision"] = ragas_metrics_df.iloc[idx].get("context_precision", 0.0)
            result_row["context_recall"] = ragas_metrics_df.iloc[idx].get("context_recall", 0.0)
        else:
            result_row["relevance"] = 0.0
            result_row["faithfulness"] = 0.0
            result_row["context_precision"] = 0.0
            result_row["context_recall"] = 0.0
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (completeness, off_topic_rate) –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
        cited_sources = ex.get("cited_sources", [])
        
        # –ï—Å–ª–∏ Ragas —É–∂–µ –≤—ã—á–∏—Å–ª–∏–ª faithfulness, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        # –ò–Ω–∞—á–µ –¥–µ–ª–∞–µ–º –ø–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
        if result_row.get("faithfulness", 0.0) > 0.0:
            # Ragas —É–∂–µ –≤—ã—á–∏—Å–ª–∏–ª faithfulness, –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –¥–ª—è completeness –∏ off_topic_rate
            # –ù–æ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–µ–ª–∞–µ–º –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—Å–µ—Ö –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            # (–º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–ª—å—à–µ, –Ω–æ —ç—Ç–æ —É—Å–ª–æ–∂–Ω–∏—Ç –∫–æ–¥)
            _, _, completeness, off_topic_rate, has_error = score_all_metrics(
                judge_client, ex["question"], ex["answer"], ex["contexts"], cited_sources
            )
        else:
            # Ragas –Ω–µ –≤—ã—á–∏—Å–ª–∏–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
            relevance_custom, faithfulness_custom, completeness, off_topic_rate, has_error = score_all_metrics(
                judge_client, ex["question"], ex["answer"], ex["contexts"], cited_sources
            )
            result_row["relevance"] = relevance_custom
            result_row["faithfulness"] = faithfulness_custom
        
        result_row["completeness"] = completeness
        result_row["off_topic_rate"] = off_topic_rate
        result_row["judge_errors"] = has_error
        
        results.append(result_row)
        
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∂–¥—ã–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π
        if (idx + 1) % 10 == 0:
            _print_intermediate_metrics(results, idx, len(examples))
    
    return pd.DataFrame(results)


def _run_custom_evaluation(
    examples: List[Dict[str, Any]],
    judge_config: Dict[str, Any]
) -> pd.DataFrame:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —á–µ—Ä–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (fallback).
    """
    logger.info(f"–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    judge_client = JudgeClient(
        provider=judge_config.get("provider", "qwen"),
        model=judge_config.get("model", ""),
        api_key=judge_config.get("api_key"),
        temperature=judge_config.get("temperature", 0.0),
        max_retries=judge_config.get("max_retries", 2),
        timeout=judge_config.get("timeout", 30)
    )
    
    results = []
    
    for idx, ex in enumerate(examples):
        result_row = {
            "query_id": idx,
            "question": ex["question"],
            "n_contexts": len(ex["contexts"]),
            "latency_ms": ex.get("latency_ms", 0.0)
        }
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ judge LLM
        cited_sources = ex.get("cited_sources", [])
        relevance_score, faithfulness_score, completeness_score, off_topic_rate, has_error = score_all_metrics(
            judge_client, ex["question"], ex["answer"], ex["contexts"], cited_sources
        )
        
        result_row["relevance"] = relevance_score
        result_row["faithfulness"] = faithfulness_score
        result_row["completeness"] = completeness_score
        result_row["off_topic_rate"] = off_topic_rate
        
        # Context precision –∏ recall (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏)
        result_row["context_precision"] = faithfulness_score  # –ò—Å–ø–æ–ª—å–∑—É–µ–º faithfulness –∫–∞–∫ proxy
        result_row["context_recall"] = min(1.0, len(ex["contexts"]) / 5.0)  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
        
        result_row["judge_errors"] = has_error
        
        results.append(result_row)
        
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∂–¥—ã–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π
        if (idx + 1) % 10 == 0:
            _print_intermediate_metrics(results, idx, len(examples))
    
    return pd.DataFrame(results)

