{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "print(f\"üìÅ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tplexity.generation.prompts import SYSTEM_PROMPT_WITH_RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tplexity.llm_client import get_llm\n",
    "\n",
    "print(\"‚úÖ –ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": SYSTEM_PROMPT_WITH_RETRIEVER,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"–ü—Ä–∏–≤–µ—Ç!\",\n",
    "    },\n",
    "]\n",
    "\n",
    "GENERATION_PARAMS = {\"temperature\": 0.7, \"max_tokens\": 500}\n",
    "\n",
    "print(\"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:\")\n",
    "print(f\"–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {TEST_MESSAGES[0]['content']}\")\n",
    "print(f\"–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {TEST_MESSAGES[1]['content']}\")\n",
    "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {GENERATION_PARAMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_provider(\n",
    "    provider: str,\n",
    "    messages: list[dict],\n",
    "    generation_params: dict,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ\n",
    "\n",
    "    Args:\n",
    "        provider (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞\n",
    "        messages (list[dict]): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI (—Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç + —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)\n",
    "        generation_params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "\n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    result = {\"provider\": provider, \"status\": \"unknown\", \"response\": None, \"time\": 0, \"error\": None}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        print(f\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {provider.upper()}\")\n",
    "\n",
    "        try:\n",
    "            client = get_llm(provider)\n",
    "            print(f\"‚úÖ –ö–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model={client.model}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞: {e}\"\n",
    "            result[\"error\"] = error_msg\n",
    "            result[\"status\"] = \"failed\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            response = await client.generate(\n",
    "                messages=messages,\n",
    "                temperature=generation_params.get(\"temperature\"),\n",
    "                max_tokens=generation_params.get(\"max_tokens\"),\n",
    "            )\n",
    "\n",
    "            result[\"response\"] = response\n",
    "            result[\"status\"] = \"success\"\n",
    "            result[\"time\"] = time.time() - start_time\n",
    "\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞ {result['time']:.2f}—Å\")\n",
    "            print(f\"–û—Ç–≤–µ—Ç: {response}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"error\"] = str(e)\n",
    "            result[\"time\"] = time.time() - start_time\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"failed\"\n",
    "        result[\"error\"] = f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\"\n",
    "        result[\"time\"] = time.time() - start_time\n",
    "        print(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROVIDERS = [\"qwen\", \"yandexgpt\", \"chatgpt\", \"deepseek\"]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for provider in PROVIDERS:\n",
    "    result = await test_provider(\n",
    "        provider=provider,\n",
    "        messages=TEST_MESSAGES,\n",
    "        generation_params=GENERATION_PARAMS,\n",
    "    )\n",
    "    all_results[provider] = result\n",
    "    print()\n",
    "\n",
    "print(\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "tplexity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
