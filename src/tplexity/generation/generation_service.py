import asyncio
import logging
import time
from datetime import datetime

import httpx

from tplexity.generation.config import settings
from tplexity.generation.llm_client import LLMClient
from tplexity.generation.memory_service import MemoryService
from tplexity.generation.prompts import (
    QUERY_REFORMULATION_PROMPT,
    REACT_DECISION_PROMPT,
    RELEVANCE_EVALUATOR_PROMPT,
    SHORT_ANSWER_PROMPT,
    SYSTEM_PROMPT_WITH_RETRIEVER,
    SYSTEM_PROMPT_WITHOUT_RETRIEVER,
    USER_PROMPT,
)

logger = logging.getLogger(__name__)


class RetrieverClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Retriever API"""

    def __init__(self, base_url: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞

        Args:
            base_url: –ë–∞–∑–æ–≤—ã–π URL Retriever API (–Ω–∞–ø—Ä–∏–º–µ—Ä, http://localhost:8010)
        """
        self.base_url = base_url.rstrip("/")

        self.client = httpx.AsyncClient()

        logger.info(f"üîÑ [generation][generation_service] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫–ª–∏–µ–Ω—Ç –¥–ª—è {self.base_url}")

    async def _search_internal(
        self,
        query: str,
        top_k: int | None = None,
        top_n: int | None = None,
        use_rerank: bool = False,
        messages: list[dict[str, str]] | None = None,
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å retry)

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ä–µ—Ä–∞–Ω–∫–∞
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∞
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ reranking
            messages: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, score, text, metadata)
        """
        payload = {
            "query": query,
            "use_rerank": use_rerank,
        }

        if top_k is not None:
            payload["top_k"] = top_k
        if top_n is not None:
            payload["top_n"] = top_n
        if messages is not None:
            payload["messages"] = messages

        response = await self.client.post(f"{self.base_url}/v1/retriever/search", json=payload)
        response.raise_for_status()

        data = response.json()
        results = data.get("results", [])

        return [(r["doc_id"], r["score"], r["text"], r.get("metadata")) for r in results]

    async def search(
        self,
        query: str,
        top_k: int | None = None,
        top_n: int | None = None,
        use_rerank: bool = False,
        messages: list[dict[str, str]] | None = None,
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ä–µ—Ä–∞–Ω–∫–∞
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∞
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ reranking
            messages: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, score, text, metadata)
        """
        try:
            results = await self._search_internal(
                query=query,
                top_k=top_k,
                top_n=top_n,
                use_rerank=use_rerank,
                messages=messages,
            )
            return results
        except httpx.HTTPStatusError as e:
            logger.error(f"‚ùå [generation][generation_service] HTTP –æ—à–∏–±–∫–∞ –æ—Ç Retriever API: {e.response.status_code}")
            raise
        except Exception as e:
            logger.error(f"‚ùå [generation][generation_service] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Retriever API: {e}")
            raise

    async def close(self) -> None:
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Retriever API"""
        await self.client.aclose()
        logger.info("üîå [generation][generation_service] –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Retriever API –∑–∞–∫—Ä—ã—Ç–æ")


class GenerationService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG (Retrieval-Augmented Generation)

    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –ü–æ–ª—É—á–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    2. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç RetrieverService –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    3. –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
    """

    def __init__(
        self,
        llm_provider: str | None = None,
        retriever_url: str | None = None,
        llm_client_url: str | None = None,
        memory_service: MemoryService | None = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Args:
            llm_provider (str | None): –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ config)
            retriever_url (str | None): URL Retriever API (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ config)
            llm_client_url (str | None): URL LLM Client API (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ config)
            memory_service (MemoryService | None): –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é –¥–∏–∞–ª–æ–≥–æ–≤
        """
        logger.info("üîÑ [generation][generation_service] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")

        retriever_url = retriever_url or settings.retriever_api_url
        self.retriever_client = RetrieverClient(retriever_url)

        llm_client_url = llm_client_url or settings.llm_client_api_url
        self.llm_client = LLMClient(llm_client_url)

        self.llm_provider = llm_provider or settings.llm_provider
        self.router_llm_provider = settings.router_llm_provider

        self.memory_service = memory_service or MemoryService()

        logger.info(
            f"‚úÖ [generation][generation_service] –°–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: provider={self.llm_provider}"
        )

    def _get_agent_provider(self, override_provider: str | None = None) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM –¥–ª—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ (—Ä–æ—É—Ç–µ—Ä, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤—â–∏–∫)
        –∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ deepseek –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.
        """

        provider = override_provider or self.router_llm_provider
        if provider == self.llm_provider:
            provider = self.router_llm_provider

        return provider

    async def _should_use_retriever(
        self, query: str, session_id: str | None = None, llm_provider: str | None = None
    ) -> bool:
        """
        ReAct –∞–≥–µ–Ω—Ç: —Ä–µ—à–∞–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ retriever –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å

        Args:
            query (str): –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            session_id (str | None): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
            llm_provider (str | None): –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è

        Returns:
            bool: True –µ—Å–ª–∏ –Ω—É–∂–µ–Ω retriever, False –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω
        """

        history_text = "–ò—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ –Ω–µ—Ç."
        if session_id:
            history = await self.memory_service.get_history(session_id)
            if history:
                history_messages = []
                for message in history:
                    role = message.get("role", "unknown")
                    content = message.get("content", "")
                    if role == "user":
                        history_messages.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {content}")
                    elif role == "assistant":
                        history_messages.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {content}")
                history_text = "\n".join(history_messages) if history_messages else "–ò—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ –Ω–µ—Ç."

        decision_prompt = REACT_DECISION_PROMPT.format(history=history_text, query=query)

        agent_provider = self._get_agent_provider(llm_provider)

        messages = [{"role": "user", "content": decision_prompt}]

        try:
            decision = await self.llm_client.generate(
                provider=agent_provider, messages=messages, temperature=0.0, max_tokens=10
            )
            decision = decision.strip().upper()

            use_retriever = decision.startswith("YES")
            return use_retriever
        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è [generation][generation_service] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–Ω—è—Ç–∏–∏ —Ä–µ—à–µ–Ω–∏—è ReAct –∞–≥–µ–Ω—Ç–æ–º: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è retriever –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."
            )
            return True

    async def _reformulate_query(
        self, query: str, session_id: str | None = None, llm_provider: str | None = None
    ) -> str:
        """
        –ê–≥–µ–Ω—Ç –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏: –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ —Ñ–æ—Ä–º—É, —É–¥–æ–±–Ω—É—é –¥–ª—è –ø–æ–∏—Å–∫–∞

        Args:
            query (str): –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            session_id (str | None): –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
            llm_provider (str | None): –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            str: –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        """

        history_text = ""
        if session_id:
            history = await self.memory_service.get_history(session_id)
            if history:
                history_messages = []
                for message in history:
                    role = message.get("role", "unknown")
                    content = message.get("content", "")
                    if role == "user":
                        history_messages.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {content}")
                    elif role == "assistant":
                        history_messages.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {content}")
                if history_messages:
                    history_text = "\n".join(history_messages[-6:])

        reformulation_prompt = QUERY_REFORMULATION_PROMPT.format(history=history_text, query=query)

        agent_provider = self._get_agent_provider(llm_provider)

        messages = [{"role": "user", "content": reformulation_prompt}]

        try:
            reformulated_query = await self.llm_client.generate(
                provider=agent_provider, messages=messages, temperature=0.0, max_tokens=200
            )
            reformulated_query = reformulated_query.strip()
            logger.info(
                f"‚úÖ [generation][generation_service] –ó–∞–ø—Ä–æ—Å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω: '{query[:50]}...' -> '{reformulated_query[:50]}...'"
            )
            return reformulated_query
        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è [generation][generation_service] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å."
            )
            return query

    async def _evaluate_document_relevance(
        self, reformulated_query: str, document_text: str, llm_provider: str | None = None
    ) -> bool:
        """
        –ê–≥–µ–Ω—Ç-–æ—Ü–µ–Ω—â–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: –±–∏–Ω–∞—Ä–Ω–æ —Ä–µ—à–∞–µ—Ç, —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É

        Args:
            reformulated_query (str): –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            document_text (str): –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            llm_provider (str | None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∞–≥–µ–Ω—Ç-–æ—Ü–µ–Ω—â–∏–∫ –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen

        Returns:
            bool: True –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, False –µ—Å–ª–∏ –Ω–µ—Ç
        """
        evaluator_prompt = RELEVANCE_EVALUATOR_PROMPT.format(
            reformulated_query=reformulated_query, document_text=document_text
        )

        messages = [{"role": "user", "content": evaluator_prompt}]

        try:
            decision = await self.llm_client.generate(
                provider="qwen", messages=messages, temperature=0.0, max_tokens=10
            )
            decision = decision.strip().upper()
            is_relevant = decision.startswith("YES")
            return is_relevant
        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è [generation][generation_service] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}. –î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."
            )
            return True

    async def _evaluate_documents_relevance_parallel(
        self,
        reformulated_query: str,
        documents: list[tuple[str, float, str, dict | None]],
        llm_provider: str | None = None,
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞-–æ—Ü–µ–Ω—â–∏–∫–∞

        Args:
            reformulated_query (str): –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            documents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, score, text, metadata)
            llm_provider (str | None): –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∞–≥–µ–Ω—Ç-–æ—Ü–µ–Ω—â–∏–∫ –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not documents:
            return []

        tasks = [self._evaluate_document_relevance(reformulated_query, text, None) for _, _, text, _ in documents]

        relevance_results = await asyncio.gather(*tasks, return_exceptions=True)

        relevant_documents = []
        for idx, (doc_id, score, text, metadata) in enumerate(documents):
            if isinstance(relevance_results[idx], Exception):
                logger.warning(
                    f"‚ö†Ô∏è [generation][generation_service] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {relevance_results[idx]}. –î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º."
                )
                relevant_documents.append((doc_id, score, text, metadata))
            elif relevance_results[idx]:
                relevant_documents.append((doc_id, score, text, metadata))
            else:
                logger.debug(f"üîç [generation][generation_service] –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –ø—Ä–∏–∑–Ω–∞–Ω –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º")

        logger.info(
            f"‚úÖ [generation][generation_service] –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(relevant_documents)}/{len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã"
        )
        return relevant_documents

    def _validate_documents(
        self, documents: list[tuple[str, float, str, dict | None]], min_score: float = 0.0, min_text_length: int = 10
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤—É

        Args:
            documents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, score, text, metadata)
            min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            min_text_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            list[tuple[str, float, str, dict | None]]: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        validated = []
        for doc_id, score, text, metadata in documents:
            if score < min_score:
                logger.debug(
                    f"üîç [generation][generation_service] –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: score {score:.3f} < {min_score}"
                )
                continue

            if not text or not isinstance(text, str):
                logger.debug(
                    f"üîç [generation][generation_service] –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç"
                )
                continue

            if len(text.strip()) < min_text_length:
                logger.debug(
                    f"üîç [generation][generation_service] –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ {len(text)} < {min_text_length}"
                )
                continue

            validated.append((doc_id, score, text, metadata))

        if len(validated) < len(documents):
            logger.info(
                f"üîç [generation][generation_service] –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)} -> {len(validated)} "
                f"(–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(documents) - len(validated)})"
            )

        return validated

    def _build_prompt(self, query: str, context_documents: list[tuple[str, float, str, dict | None]]) -> str:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è LLM

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_documents: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, score, text, metadata)

        Returns:
            str: –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """

        context_parts = []
        for idx, (_doc_id, score, text, _metadata) in enumerate(context_documents, 1):
            context_parts.append(f"[{idx}] –î–æ–∫—É–º–µ–Ω—Ç {idx} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\n{text}")

        context = "\n\n".join(context_parts)

        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return USER_PROMPT.format(context=context, query=query, current_time=current_time)

    async def _call_llm(
        self,
        provider: str,
        messages: list[dict[str, str]],
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> str:
        """
        –í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ LLMClient

        Args:
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ settings.llm)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ settings.llm)

        Returns:
            str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        logger.debug("üîÑ [generation][generation_service] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM")
        return await self.llm_client.generate(
            provider=provider, messages=messages, temperature=temperature, max_tokens=max_tokens
        )

    async def generate(
        self,
        query: str,
        top_k: int | None = None,
        use_rerank: bool | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        llm_provider: str | None = None,
        session_id: str | None = None,
    ) -> tuple[str, list[str], list[dict | None], float | None, float, float]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ retriever config)
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ reranking (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ llm config)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ llm config)
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ self.llm_provider)
            session_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Å—Å–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ (–µ—Å–ª–∏ None, –∏—Å—Ç–æ—Ä–∏—è –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è)

        Returns:
            tuple[str, list[str], list[dict | None], float | None, float, float]:
            (–æ—Ç–≤–µ—Ç, —Å–ø–∏—Å–æ–∫ doc_ids, —Å–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞, –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ–±—â–µ–µ –≤—Ä–µ–º—è)

        Raises:
            ValueError: –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç
        """
        if not query or not query.strip():
            raise ValueError("–ó–∞–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        total_start_time = time.time()

        use_rerank = use_rerank if use_rerank is not None else True

        provider = llm_provider or self.llm_provider
        logger.info(f"üîÑ [generation][generation_service] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...'")

        react_start_time = time.time()
        use_retriever = await self._should_use_retriever(query, session_id, llm_provider)
        react_time = time.time() - react_start_time
        logger.info(
            f"‚úÖ [generation][generation_service] ReAct –∞–≥–µ–Ω—Ç: {'–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å' if use_retriever else '–ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å'} retriever ({react_time:.2f}—Å)"
        )

        context_documents = []
        search_time = None
        if use_retriever:
            reformulation_start_time = time.time()
            reformulated_query = await self._reformulate_query(query, session_id, llm_provider)
            reformulation_time = time.time() - reformulation_start_time
            logger.info(
                f"‚úÖ [generation][generation_service] –ê–≥–µ–Ω—Ç –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏: –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω –∑–∞ {reformulation_time:.2f}—Å"
            )

            search_start_time = time.time()
            raw_documents = await self.retriever_client.search(
                query=reformulated_query, top_k=top_k, top_n=None, use_rerank=use_rerank, messages=None
            )
            retrieval_time = time.time() - search_start_time
            logger.info(
                f"‚úÖ [generation][generation_service] Retriever: –Ω–∞–π–¥–µ–Ω–æ {len(raw_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ {retrieval_time:.2f}—Å"
            )

            validated_documents = self._validate_documents(raw_documents, min_score=0.0, min_text_length=10)

            if not validated_documents:
                logger.warning("‚ö†Ô∏è [generation][generation_service] –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏ –±–∞–∑–æ–≤—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                error_message = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
                total_time = time.time() - total_start_time
                return (
                    error_message,
                    [],
                    [],
                    time.time() - search_start_time,
                    0.0,
                    total_time,
                )

            evaluation_start_time = time.time()
            context_documents = await self._evaluate_documents_relevance_parallel(
                reformulated_query, validated_documents, llm_provider
            )
            evaluation_time = time.time() - evaluation_start_time
            search_time = time.time() - search_start_time
            logger.info(
                f"‚úÖ [generation][generation_service] –ê–≥–µ–Ω—Ç-–æ—Ü–µ–Ω—â–∏–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {len(context_documents)}/{len(validated_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –∑–∞ {evaluation_time:.2f}—Å"
            )

            if not context_documents:
                logger.warning("‚ö†Ô∏è [generation][generation_service] –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –æ—Ü–µ–Ω–∫–∏")
                error_message = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
                total_time = time.time() - total_start_time
                return (
                    error_message,
                    [],
                    [],
                    search_time,
                    0.0,
                    total_time,
                )

        if context_documents:
            prompt = self._build_prompt(query, context_documents)
        else:
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            prompt = f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}\n\n–¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {current_time}"

        system_prompt = SYSTEM_PROMPT_WITH_RETRIEVER if context_documents else SYSTEM_PROMPT_WITHOUT_RETRIEVER

        messages = [{"role": "system", "content": system_prompt}]

        if session_id:
            history = await self.memory_service.get_history(session_id)
            if history:
                history_messages = [message for message in history if message.get("role") in ("user", "assistant")]
                for message in history_messages:
                    messages.append({"role": message.get("role"), "content": message.get("content", "")})
                if history_messages:
                    logger.debug(
                        f"üìö [generation][generation_service] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è: {len(history_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π"
                    )

        messages.append({"role": "user", "content": prompt})

        provider = llm_provider or self.llm_provider

        generation_start_time = time.time()
        answer = await self.llm_client.generate(
            provider=provider, messages=messages, temperature=temperature, max_tokens=max_tokens
        )
        generation_time = time.time() - generation_start_time
        logger.info(
            f"‚úÖ [generation][generation_service] –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {generation_time:.2f}—Å (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider})"
        )

        if session_id:
            try:
                await self.memory_service.add_message(session_id, "user", query)
                await self.memory_service.add_message(session_id, "assistant", answer)

                await self.memory_service.update_ttl(session_id)
                logger.debug(f"üíæ [generation][generation_service] –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")
            except Exception as e:
                logger.error(
                    f"‚ùå [generation][generation_service] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}: {e}"
                )

        doc_ids = [doc_id for doc_id, _, _, _ in context_documents]
        metadatas = [metadata for _, _, _, metadata in context_documents]

        total_time = time.time() - total_start_time
        search_str = f"{search_time:.2f}—Å" if search_time is not None else "N/A"
        logger.info(
            f"‚úÖ [generation][generation_service] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f}—Å (–ø–æ–∏—Å–∫: {search_str}, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {generation_time:.2f}—Å)"
        )

        return answer, doc_ids, metadatas, search_time, generation_time, total_time

    async def generate_short_answer(
        self,
        detailed_answer: str,
        llm_provider: str | None = None,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.

        Args:
            detailed_answer: –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ self.llm_provider)

        Returns:
            str: –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
        """

        provider = llm_provider or self.llm_provider
        logger.info(f"üîÑ [generation][generation_service] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider})")

        prompt = SHORT_ANSWER_PROMPT.format(detailed_answer=detailed_answer)

        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî –∞–≥–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫—Ä–∞—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã RAG."},
            {"role": "user", "content": prompt},
        ]

        provider = llm_provider or self.llm_provider

        short_answer = await self.llm_client.generate(provider=provider, messages=messages)
        logger.info("‚úÖ [generation][generation_service] –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")

        return short_answer

    async def clear_session(self, session_id: str) -> None:
        """
        –û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏

        Args:
            session_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Å—Å–∏–∏
        """
        await self.memory_service.clear_history(session_id)

    async def close(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ LLM –∫–ª–∏–µ–Ω—Ç–∞, Retriever –∫–ª–∏–µ–Ω—Ç–∞ –∏ —Å–µ—Ä–≤–∏—Å–∞ –ø–∞–º—è—Ç–∏"""
        if hasattr(self, "retriever_client"):
            await self.retriever_client.close()
        if hasattr(self, "llm_client"):
            await self.llm_client.close()
        if hasattr(self, "memory_service"):
            await self.memory_service.close()
