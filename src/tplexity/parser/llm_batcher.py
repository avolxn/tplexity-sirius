import asyncio
import hashlib
import logging
from dataclasses import dataclass, field

from tplexity.parser.llm_client import LLMClient, get_llm_client

logger = logging.getLogger(__name__)


@dataclass
class LLMRequest:
    """–ó–∞–ø—Ä–æ—Å –∫ LLM —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º"""

    post_text: str
    llm_provider: str
    future: asyncio.Future = field(default_factory=asyncio.Future)
    cache_key: str = field(default="")

    def __post_init__(self):
        """–í—ã—á–∏—Å–ª—è–µ–º cache_key –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if not self.cache_key:
            text_hash = hashlib.md5(self.post_text.encode()).hexdigest() # noqa: S324
            self.cache_key = f"{self.llm_provider}:{text_hash}"


class LLMBatcher:
    """
    –ë–∞—Ç—á–µ—Ä –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ LLM-–∑–∞–ø—Ä–æ—Å–æ–≤.

    –°–æ–±–∏—Ä–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –≤ –±–∞—Ç—á–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """

    def __init__(
        self,
        batch_size: int = 5,
        batch_timeout: float = 0.5,
        max_cache_size: int = 1000,
        llm_provider: str = "qwen",
        llm_client: LLMClient | None = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–µ—Ä–∞

        Args:
            batch_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            batch_timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞ (—Å–µ–∫—É–Ω–¥—ã)
            max_cache_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            llm_client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è LLM (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
        """
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.max_cache_size = max_cache_size
        self.default_llm_provider = llm_provider
        self.llm_client = llm_client or get_llm_client()

        self.queue: asyncio.Queue[LLMRequest] = asyncio.Queue()

        self.cache: dict[str, tuple[int, str]] = {}

        self.is_running = False

        self.batch_task: asyncio.Task | None = None

    async def start(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π"""
        if self.is_running:
            logger.warning("‚ö†Ô∏è [llm_batcher] –ë–∞—Ç—á–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
            return

        self.is_running = True
        self.batch_task = asyncio.create_task(self._batch_processor())
        logger.info(
            f"‚úÖ [llm_batcher] –ë–∞—Ç—á–µ—Ä –∑–∞–ø—É—â–µ–Ω (batch_size={self.batch_size}, batch_timeout={self.batch_timeout}s)"
        )

    async def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –±–∞—Ç—á–µ—Ä"""
        self.is_running = False
        if self.batch_task:
            self.batch_task.cancel()
            try:
                await self.batch_task
            except asyncio.CancelledError:
                pass
        logger.info("üõë [llm_batcher] –ë–∞—Ç—á–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    async def determine_relevance_days(self, post_text: str, llm_provider: str | None = None) -> tuple[int, str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM —Å –±–∞—Ç—á–∏–Ω–≥–æ–º

        Args:
            post_text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default)

        Returns:
            tuple[int, str]: (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏, —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç LLM)
        """
        provider = llm_provider or self.default_llm_provider

        cache_key = f"{provider}:{hashlib.md5(post_text.encode()).hexdigest()}" # noqa: S324
        if cache_key in self.cache:
            relevance_days, raw_response = self.cache[cache_key]
            logger.debug(f"üíæ [llm_batcher] –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ –¥–ª—è –ø–æ—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(post_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return relevance_days, raw_response

        request = LLMRequest(post_text=post_text, llm_provider=provider, cache_key=cache_key)

        await self.queue.put(request)

        try:
            relevance_days, raw_response = await asyncio.wait_for(request.future, timeout=30.0)

            self._add_to_cache(cache_key, relevance_days, raw_response)
            return relevance_days, raw_response
        except TimeoutError:
            logger.error(f"‚ùå [llm_batcher] –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø–æ—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(post_text)} —Å–∏–º–≤–æ–ª–æ–≤)")

            return 30, "TIMEOUT"

    async def _batch_processor(self):
        """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π"""
        logger.info("üîÑ [llm_batcher] –ó–∞–ø—É—â–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π")

        while self.is_running:
            try:
                batch = await self._collect_batch()

                if not batch:
                    continue

                await self._process_batch(batch)

            except asyncio.CancelledError:
                logger.info("üõë [llm_batcher] –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                break
            except Exception as e:
                logger.error(f"‚ùå [llm_batcher] –û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–µ–π: {e}", exc_info=True)
                await asyncio.sleep(1)

    async def _collect_batch(self) -> list[LLMRequest]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –æ—á–µ—Ä–µ–¥–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        batch: list[LLMRequest] = []
        first_request = None

        try:
            first_request = await asyncio.wait_for(self.queue.get(), timeout=self.batch_timeout)
            batch.append(first_request)
        except TimeoutError:
            return []

        batch_timeout = self.batch_timeout
        while len(batch) < self.batch_size:
            try:
                request = await asyncio.wait_for(self.queue.get(), timeout=batch_timeout)
                batch.append(request)

                batch_timeout = 0.1
            except TimeoutError:
                break

        logger.debug(f"üì¶ [llm_batcher] –°–æ–±—Ä–∞–Ω –±–∞—Ç—á –∏–∑ {len(batch)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        return batch

    async def _process_batch(self, batch: list[LLMRequest]):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤

        Args:
            batch: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        if not batch:
            return

        requests_by_provider: dict[str, list[LLMRequest]] = {}
        for request in batch:
            provider = request.llm_provider
            if provider not in requests_by_provider:
                requests_by_provider[provider] = []
            requests_by_provider[provider].append(request)

        tasks = []
        for provider, provider_requests in requests_by_provider.items():
            task = self._process_provider_batch(provider, provider_requests)
            tasks.append(task)

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_provider_batch(self, provider: str, requests: list[LLMRequest]):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

        Args:
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM
            requests: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        try:
            tasks = []
            for request in requests:
                task = self._process_single_request(request, provider)
                tasks.append(task)

            await asyncio.gather(*tasks, return_exceptions=True)

        except Exception as e:
            logger.error(f"‚ùå [llm_batcher] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ {provider}: {e}", exc_info=True)

            for request in requests:
                if not request.future.done():
                    request.future.set_exception(e)

    async def _process_single_request(self, request: LLMRequest, provider: str):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ LLM

        Args:
            request: –ó–∞–ø—Ä–æ—Å
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM
        """
        try:
            from tplexity.parser.relevance_analyzer import RELEVANCE_PROMPT

            messages = [
                {
                    "role": "user",
                    "content": RELEVANCE_PROMPT.format(post_text=request.post_text),
                }
            ]

            raw_response = await self.llm_client.generate(
                provider=provider,
                messages=messages,
                temperature=0.0,
                max_tokens=50,
            )

            response = raw_response.strip()
            digits = ""
            for char in response:
                if char.isdigit():
                    digits += char
                elif digits:
                    break

            if not digits:
                logger.warning(
                    f"‚ö†Ô∏è [llm_batcher] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM: {response}, "
                    f"–∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30"
                )
                relevance_days = 30
            else:
                relevance_days = int(digits)

                relevance_days = max(1, min(10000, relevance_days))

            if not request.future.done():
                request.future.set_result((relevance_days, raw_response))

            logger.debug(
                f"‚úÖ [llm_batcher] –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: {relevance_days} –¥–Ω–µ–π "
                f"–¥–ª—è –ø–æ—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(request.post_text)} —Å–∏–º–≤–æ–ª–æ–≤)"
            )

        except Exception as e:
            logger.error(
                f"‚ùå [llm_batcher] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}",
                exc_info=True,
            )

            if not request.future.done():
                request.future.set_result((30, f"ERROR: {str(e)}"))

    def _add_to_cache(self, cache_key: str, relevance_days: int, raw_response: str):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞

        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            relevance_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
            raw_response: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç LLM
        """

        if len(self.cache) >= self.max_cache_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[cache_key] = (relevance_days, raw_response)


_batcher_instance: LLMBatcher | None = None


def get_batcher(llm_provider: str = "qwen", llm_client: LLMClient | None = None) -> LLMBatcher:
    """
    –ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∞—Ç—á–µ—Ä–∞ (singleton)

    Args:
        llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        llm_client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è LLM (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)

    Returns:
        LLMBatcher: –≠–∫–∑–µ–º–ø–ª—è—Ä –±–∞—Ç—á–µ—Ä–∞

    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
        –ë–∞—Ç—á–µ—Ä –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ await batcher.start()
        –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
    """
    global _batcher_instance

    if _batcher_instance is None:
        _batcher_instance = LLMBatcher(llm_provider=llm_provider, llm_client=llm_client)

    return _batcher_instance
