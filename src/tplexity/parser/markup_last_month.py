import asyncio
import logging
from datetime import UTC, datetime, timedelta
from pathlib import Path

import httpx

from tplexity.parser.config import settings
from tplexity.parser.llm_batcher import get_batcher
from tplexity.parser.relevance_analyzer import calculate_delete_date
from tplexity.parser.telegram_downloader import TelegramDownloader

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


async def clear_database(retriever_url: str) -> bool:
    """
    –û—á–∏—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î, —É–¥–∞–ª—è—è –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.

    Args:
        retriever_url: URL retriever API

    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
    """
    try:
        delete_url = f"{retriever_url.rstrip('/')}/v1/retriever/documents/all"
        logger.info(f"üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {delete_url}")

        async with httpx.AsyncClient() as client:
            response = await client.delete(delete_url)
            response.raise_for_status()

        logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞")
        return True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –ë–î: {e}")
        return False


async def send_posts_to_retriever(
    posts: list[dict],
    channel: str,
    retriever_url: str,
    llm_batcher,
    llm_provider: str,
    batch_size: int = 50,
    channel_titles: dict[str, str] | None = None,
) -> tuple[int, int]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç—ã –≤ retriever —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ LLM.

    Args:
        posts: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        channel: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
        retriever_url: URL retriever API
        llm_batcher: –ë–∞—Ç—á–µ—Ä –¥–ª—è LLM –∑–∞–ø—Ä–æ—Å–æ–≤
        llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        channel_titles: –°–ª–æ–≤–∞—Ä—å —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–∞–Ω–∞–ª–æ–≤

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ, –æ—à–∏–±–æ–∫)
    """
    if not posts:
        return 0, 0

    documents_url = f"{retriever_url.rstrip('/')}/v1/retriever/documents"
    success_count = 0
    error_count = 0

    async with httpx.AsyncClient() as http_client:
        for i in range(0, len(posts), batch_size):
            batch = posts[i : i + batch_size]
            prepared_posts: list[dict] = []
            llm_tasks = []

            for post in batch:
                text = (post.get("text") or "").strip()
                if not text:
                    continue

                date_str = post.get("date")
                post_date = None
                if date_str:
                    try:
                        if date_str.endswith("Z"):
                            date_str = date_str.replace("Z", "+00:00")

                        if "T" in date_str:
                            post_date = datetime.fromisoformat(date_str)
                        else:
                            post_date = datetime.fromisoformat(f"{date_str}T00:00:00")

                        formatted_date = post_date.strftime("%Y-%m-%d %H:%M:%S")
                        text = f"{text}\n\n{formatted_date}"
                    except (ValueError, AttributeError) as e:
                        logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str}, –æ—à–∏–±–∫–∞: {e}")

                metadata = {k: v for k, v in post.items() if k != "text"}
                metadata["channel_name"] = channel

                if channel_titles:
                    channel_title = channel_titles.get(channel, channel)
                    metadata["channel_title"] = channel_title
                else:
                    metadata["channel_title"] = channel

                prepared_posts.append(
                    {
                        "text": text,
                        "metadata": metadata,
                        "post_date": post_date,
                        "original_post_id": post.get("id"),
                    }
                )
                llm_tasks.append(llm_batcher.determine_relevance_days(text, llm_provider))

            if not prepared_posts:
                continue

            llm_results = await asyncio.gather(*llm_tasks, return_exceptions=True)

            documents = []
            for prepared, result in zip(prepared_posts, llm_results, strict=False):
                if isinstance(result, Exception):
                    logger.warning(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ {prepared.get('original_post_id')}: {result}"
                    )
                    documents.append({"text": prepared["text"], "metadata": prepared["metadata"]})
                    continue

                relevance_days, _ = result
                delete_date = calculate_delete_date(relevance_days, prepared["post_date"])
                prepared["metadata"]["delete_date"] = delete_date
                documents.append({"text": prepared["text"], "metadata": prepared["metadata"]})

            if not documents:
                continue

            try:
                response = await http_client.post(documents_url, json={"documents": documents})
                response.raise_for_status()
                success_count += len(documents)
                logger.info(
                    f"üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {len(documents)} –ø–æ—Å—Ç–æ–≤ –∏–∑ {channel} "
                    f"(–±–∞—Ç—á {i // batch_size + 1}/{(len(posts) + batch_size - 1) // batch_size})"
                )
            except Exception as e:
                error_count += len(documents)
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –±–∞—Ç—á–∞ –∏–∑ {channel}: {e}")

    return success_count, error_count


async def markup_last_month(days: int = 14):
    """
    –†–∞–∑–º–µ—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π.

    Args:
        days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 14 - 2 –Ω–µ–¥–µ–ª–∏)
    """
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –Ω–µ–¥–µ–ª–∏")
    logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π")

    if not settings.api_id or not settings.api_hash:
        logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã API_ID –∏–ª–∏ API_HASH –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        return

    channels_list = settings.get_channels_list()
    if not channels_list:
        logger.error("‚ùå –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –ø—É—Å—Ç")
        return

    if not settings.webhook_url:
        logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω WEBHOOK_URL –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        return

    retriever_url = settings.webhook_url.rsplit("/retriever", 1)[0]
    logger.info(f"üì° Retriever URL: {retriever_url}")
    logger.info(f"üìã –ö–∞–Ω–∞–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {', '.join(channels_list)}")

    logger.info("=" * 60)
    logger.info("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –ø–µ—Ä–µ–¥ —Ä–∞–∑–º–µ—Ç–∫–æ–π...")
    if not await clear_database(retriever_url):
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –ë–î, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ")
        return
    logger.info("=" * 60)

    days_ago = datetime.now(UTC) - timedelta(days=days)
    logger.info(f"üìÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã —Å {days_ago.strftime('%Y-%m-%d %H:%M:%S UTC')}")

    project_root = Path(__file__).parent.parent.parent.parent
    session_path = project_root / settings.session_name

    logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ TelegramDownloader...")
    downloader = TelegramDownloader(
        api_id=settings.api_id,
        api_hash=settings.api_hash,
        session_name=str(session_path),
        session_string=settings.session_string,
        download_path=str(project_root / settings.data_dir / "telegram"),
    )

    llm_batcher = get_batcher(settings.llm_provider)
    await llm_batcher.start()
    logger.info("‚úÖ LLM –±–∞—Ç—á–µ—Ä –∑–∞–ø—É—â–µ–Ω")

    try:
        logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Telegram...")
        try:
            await downloader.client.connect()
            logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Telegram —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ Telegram: {e}", exc_info=True)
            return

        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏...")
        is_authorized = await downloader.client.is_user_authorized()
        logger.info(f"üîç –°—Ç–∞—Ç—É—Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {is_authorized}")

        if not is_authorized:
            error_msg = (
                "Telegram –∫–ª–∏–µ–Ω—Ç –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è.\n"
                f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {'—Å—Ç—Ä–æ–∫–∞ —Å–µ—Å—Å–∏–∏' if settings.session_string else f'—Ñ–∞–π–ª —Å–µ—Å—Å–∏–∏ ({session_path})'}\n"
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç: poetry run python src/tplexity/parser/authorize_telegram.py"
            )
            logger.error(f"‚ùå {error_msg}")
            return

        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Telegram –∏ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ")

        total_posts_downloaded = 0
        total_posts_sent = 0
        total_errors = 0

        channel_titles: dict[str, str] = {}
        for channel in channels_list:
            try:
                entity = await downloader.client.get_entity(channel)
                channel_title = getattr(entity, "title", None) or channel
                channel_titles[channel] = channel_title
                logger.info(f"üì∫ –ö–∞–Ω–∞–ª {channel}: –Ω–∞–∑–≤–∞–Ω–∏–µ '{channel_title}'")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞ {channel}: {e}")
                channel_titles[channel] = channel

        for channel_idx, channel in enumerate(channels_list, 1):
            logger.info(f"\n{'=' * 60}\nüì• –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞ {channel_idx}/{len(channels_list)}: {channel}\n{'=' * 60}")

            try:
                logger.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –∏–∑ {channel}...")
                all_messages = []

                async for message in downloader.client.iter_messages(
                    channel,
                    limit=None,
                    offset_date=None,
                    reverse=False,
                ):
                    if not hasattr(message, "date") or not message.date:
                        continue

                    if message.date < days_ago:
                        break

                    message_dict = await downloader._message_to_dict(message, channel)
                    all_messages.append(message_dict)

                    if len(all_messages) % 50 == 0:
                        logger.info(f"  üì• –°–∫–∞—á–∞–Ω–æ {len(all_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ {channel}...")

                messages_with_text = [
                    msg
                    for msg in all_messages
                    if msg.get("text") and isinstance(msg.get("text"), str) and msg.get("text").strip()
                ]

                total_posts_downloaded += len(messages_with_text)
                logger.info(
                    f"üìä –ö–∞–Ω–∞–ª {channel}: —Å–∫–∞—á–∞–Ω–æ {len(all_messages)} –ø–æ—Å—Ç–æ–≤, {len(messages_with_text)} —Å —Ç–µ–∫—Å—Ç–æ–º"
                )

                if messages_with_text:
                    success, errors = await send_posts_to_retriever(
                        messages_with_text,
                        channel,
                        retriever_url,
                        llm_batcher,
                        settings.llm_provider,
                        channel_titles=channel_titles,
                    )
                    total_posts_sent += success
                    total_errors += errors

                    logger.info(f"‚úÖ –ö–∞–Ω–∞–ª {channel}: –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {success} –ø–æ—Å—Ç–æ–≤, –æ—à–∏–±–æ–∫: {errors}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ö–∞–Ω–∞–ª {channel}: –Ω–µ—Ç –ø–æ—Å—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–Ω–∞–ª–∞ {channel}: {e}", exc_info=True)
                total_errors += 1

        logger.info(
            f"\n{'=' * 60}\n"
            f"‚úÖ –†–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n"
            f"{'=' * 60}\n"
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n"
            f"  - –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {total_posts_downloaded}\n"
            f"  - –£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ –ë–î: {total_posts_sent}\n"
            f"  - –û—à–∏–±–æ–∫: {total_errors}\n"
            f"{'=' * 60}"
        )

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
    finally:
        await llm_batcher.stop()

        try:
            await downloader.disconnect()
            logger.info("‚úÖ –û—Ç–∫–ª—é—á–µ–Ω–æ –æ—Ç Telegram")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏: {e}")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞."""
    import sys

    days = 14
    if len(sys.argv) > 1:
        try:
            days = int(sys.argv[1])
        except ValueError:
            logger.warning(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç {sys.argv[1]}, –∏—Å–ø–æ–ª—å–∑—É–µ–º 14 –¥–Ω–µ–π (2 –Ω–µ–¥–µ–ª–∏)")

    asyncio.run(markup_last_month(days=days))


if __name__ == "__main__":
    main()
